{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Машинное обучение для текстов: классификация комментариев"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Заказчику - некому интернет-магазину - для оптимизации модерирования комментариев необходима модель, которая будет определять, токсичный ли комментарий оставил пользователь, или нет. \n",
    "\n",
    "Цель - обучить модель, которая решала бы задачу классификации, разделяя комментарии на позитивные и негативные. Для обучения модели есть размеченный набор комментариев. \n",
    "\n",
    "Заказчик требует от модели значение метрики *F1* не меньше 0.75. \n",
    "\n",
    "Столбец *text* в исходном датафрейме содержит текст комментария, *toxic* — указание, токсичный коментарий, или нет, целевой признак."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Подготовка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import copy\n",
    "import math\n",
    "import nltk\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from nltk.corpus import stopwords as nltk_stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "pip install catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Пользователь\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\wordnet.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 11.8 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "nltk.download('wordnet')\n",
    "#nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "#from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = 12345"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>\"\\n\\nCongratulations from me as well, use the ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>COCKSUCKER BEFORE YOU PISS AROUND ON MY WORK</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Your vandalism to the Matt Shirvington article...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Sorry if the word 'nonsense' was offensive to ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>alignment on this subject and which are contra...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  toxic\n",
       "0  Explanation\\nWhy the edits made under my usern...      0\n",
       "1  D'aww! He matches this background colour I'm s...      0\n",
       "2  Hey man, I'm really not trying to edit war. It...      0\n",
       "3  \"\\nMore\\nI can't make any real suggestions on ...      0\n",
       "4  You, sir, are my hero. Any chance you remember...      0\n",
       "5  \"\\n\\nCongratulations from me as well, use the ...      0\n",
       "6       COCKSUCKER BEFORE YOU PISS AROUND ON MY WORK      1\n",
       "7  Your vandalism to the Matt Shirvington article...      0\n",
       "8  Sorry if the word 'nonsense' was offensive to ...      0\n",
       "9  alignment on this subject and which are contra...      0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 159571 entries, 0 to 159570\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count   Dtype \n",
      "---  ------  --------------   ----- \n",
      " 0   text    159571 non-null  object\n",
      " 1   toxic   159571 non-null  int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 2.4+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data[data.text.isna()==True])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data[data.toxic.isna()==True])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data[data.text.duplicated()==True])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Приведение к нижнему регистру"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Приведем классифицируемый текст к нижнему регистру, это облегчит дальнейшую лемматизацию и обучение модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['original'] = data.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.text = data.text.str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>original</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>explanation\\nwhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>d'aww! he matches this background colour i'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hey man, i'm really not trying to edit war. it...</td>\n",
       "      <td>0</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"\\nmore\\ni can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>you, sir, are my hero. any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>\"\\n\\ncongratulations from me as well, use the ...</td>\n",
       "      <td>0</td>\n",
       "      <td>\"\\n\\nCongratulations from me as well, use the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>cocksucker before you piss around on my work</td>\n",
       "      <td>1</td>\n",
       "      <td>COCKSUCKER BEFORE YOU PISS AROUND ON MY WORK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>your vandalism to the matt shirvington article...</td>\n",
       "      <td>0</td>\n",
       "      <td>Your vandalism to the Matt Shirvington article...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>sorry if the word 'nonsense' was offensive to ...</td>\n",
       "      <td>0</td>\n",
       "      <td>Sorry if the word 'nonsense' was offensive to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>alignment on this subject and which are contra...</td>\n",
       "      <td>0</td>\n",
       "      <td>alignment on this subject and which are contra...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  toxic  \\\n",
       "0  explanation\\nwhy the edits made under my usern...      0   \n",
       "1  d'aww! he matches this background colour i'm s...      0   \n",
       "2  hey man, i'm really not trying to edit war. it...      0   \n",
       "3  \"\\nmore\\ni can't make any real suggestions on ...      0   \n",
       "4  you, sir, are my hero. any chance you remember...      0   \n",
       "5  \"\\n\\ncongratulations from me as well, use the ...      0   \n",
       "6       cocksucker before you piss around on my work      1   \n",
       "7  your vandalism to the matt shirvington article...      0   \n",
       "8  sorry if the word 'nonsense' was offensive to ...      0   \n",
       "9  alignment on this subject and which are contra...      0   \n",
       "\n",
       "                                            original  \n",
       "0  Explanation\\nWhy the edits made under my usern...  \n",
       "1  D'aww! He matches this background colour I'm s...  \n",
       "2  Hey man, I'm really not trying to edit war. It...  \n",
       "3  \"\\nMore\\nI can't make any real suggestions on ...  \n",
       "4  You, sir, are my hero. Any chance you remember...  \n",
       "5  \"\\n\\nCongratulations from me as well, use the ...  \n",
       "6       COCKSUCKER BEFORE YOU PISS AROUND ON MY WORK  \n",
       "7  Your vandalism to the Matt Shirvington article...  \n",
       "8  Sorry if the word 'nonsense' was offensive to ...  \n",
       "9  alignment on this subject and which are contra...  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Очистка корпуса"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Отчистим текст от лишних знаков, оставим только буквы (в данном случае - английского алфавита) и пробелы. Мы привели текст к нижнему регистру, поэтому достаточно было бы указать только диапазон знаков a-z и пробел, однако на всякий случай оставим также диапазон A-Z.  \n",
    "Для этого определим функцию и проверим ее работу на сэмпле."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_clearing(text):\n",
    "    space_text = re.sub(r'[^a-zA-Z ]',' ', text)\n",
    "    cleared_text = \" \".join(space_text.split())\n",
    "    return cleared_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Применим функцию очистки текста к корпусу."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1.52 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "clear_text = data['text'].apply(text_clearing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = copy.deepcopy(data)\n",
    "data2['clear_text'] = clear_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Токенизация корпуса"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Произведем токенизацию очищенного корпуса с помощью возможностей nltk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Пользователь\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 30.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tokenized_text = clear_text.apply(\n",
    "    lambda x: nltk.word_tokenize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         [explanation, why, the, edits, made, under, my...\n",
       "1         [d, aww, he, matches, this, background, colour...\n",
       "2         [hey, man, i, m, really, not, trying, to, edit...\n",
       "3         [more, i, can, t, make, any, real, suggestions...\n",
       "4         [you, sir, are, my, hero, any, chance, you, re...\n",
       "                                ...                        \n",
       "159566    [and, for, the, second, time, of, asking, when...\n",
       "159567    [you, should, be, ashamed, of, yourself, that,...\n",
       "159568    [spitzer, umm, theres, no, actual, article, fo...\n",
       "159569    [and, it, looks, like, it, was, actually, you,...\n",
       "159570    [and, i, really, don, t, think, you, understan...\n",
       "Name: text, Length: 159571, dtype: object"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>original</th>\n",
       "      <th>clear_text</th>\n",
       "      <th>tokenized_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>explanation\\nwhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>explanation why the edits made under my userna...</td>\n",
       "      <td>[explanation, why, the, edits, made, under, my...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>d'aww! he matches this background colour i'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>d aww he matches this background colour i m se...</td>\n",
       "      <td>[d, aww, he, matches, this, background, colour...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hey man, i'm really not trying to edit war. it...</td>\n",
       "      <td>0</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>hey man i m really not trying to edit war it s...</td>\n",
       "      <td>[hey, man, i, m, really, not, trying, to, edit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"\\nmore\\ni can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>more i can t make any real suggestions on impr...</td>\n",
       "      <td>[more, i, can, t, make, any, real, suggestions...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>you, sir, are my hero. any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>you sir are my hero any chance you remember wh...</td>\n",
       "      <td>[you, sir, are, my, hero, any, chance, you, re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>\"\\n\\ncongratulations from me as well, use the ...</td>\n",
       "      <td>0</td>\n",
       "      <td>\"\\n\\nCongratulations from me as well, use the ...</td>\n",
       "      <td>congratulations from me as well use the tools ...</td>\n",
       "      <td>[congratulations, from, me, as, well, use, the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>cocksucker before you piss around on my work</td>\n",
       "      <td>1</td>\n",
       "      <td>COCKSUCKER BEFORE YOU PISS AROUND ON MY WORK</td>\n",
       "      <td>cocksucker before you piss around on my work</td>\n",
       "      <td>[cocksucker, before, you, piss, around, on, my...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>your vandalism to the matt shirvington article...</td>\n",
       "      <td>0</td>\n",
       "      <td>Your vandalism to the Matt Shirvington article...</td>\n",
       "      <td>your vandalism to the matt shirvington article...</td>\n",
       "      <td>[your, vandalism, to, the, matt, shirvington, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>sorry if the word 'nonsense' was offensive to ...</td>\n",
       "      <td>0</td>\n",
       "      <td>Sorry if the word 'nonsense' was offensive to ...</td>\n",
       "      <td>sorry if the word nonsense was offensive to yo...</td>\n",
       "      <td>[sorry, if, the, word, nonsense, was, offensiv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>alignment on this subject and which are contra...</td>\n",
       "      <td>0</td>\n",
       "      <td>alignment on this subject and which are contra...</td>\n",
       "      <td>alignment on this subject and which are contra...</td>\n",
       "      <td>[alignment, on, this, subject, and, which, are...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  toxic  \\\n",
       "0  explanation\\nwhy the edits made under my usern...      0   \n",
       "1  d'aww! he matches this background colour i'm s...      0   \n",
       "2  hey man, i'm really not trying to edit war. it...      0   \n",
       "3  \"\\nmore\\ni can't make any real suggestions on ...      0   \n",
       "4  you, sir, are my hero. any chance you remember...      0   \n",
       "5  \"\\n\\ncongratulations from me as well, use the ...      0   \n",
       "6       cocksucker before you piss around on my work      1   \n",
       "7  your vandalism to the matt shirvington article...      0   \n",
       "8  sorry if the word 'nonsense' was offensive to ...      0   \n",
       "9  alignment on this subject and which are contra...      0   \n",
       "\n",
       "                                            original  \\\n",
       "0  Explanation\\nWhy the edits made under my usern...   \n",
       "1  D'aww! He matches this background colour I'm s...   \n",
       "2  Hey man, I'm really not trying to edit war. It...   \n",
       "3  \"\\nMore\\nI can't make any real suggestions on ...   \n",
       "4  You, sir, are my hero. Any chance you remember...   \n",
       "5  \"\\n\\nCongratulations from me as well, use the ...   \n",
       "6       COCKSUCKER BEFORE YOU PISS AROUND ON MY WORK   \n",
       "7  Your vandalism to the Matt Shirvington article...   \n",
       "8  Sorry if the word 'nonsense' was offensive to ...   \n",
       "9  alignment on this subject and which are contra...   \n",
       "\n",
       "                                          clear_text  \\\n",
       "0  explanation why the edits made under my userna...   \n",
       "1  d aww he matches this background colour i m se...   \n",
       "2  hey man i m really not trying to edit war it s...   \n",
       "3  more i can t make any real suggestions on impr...   \n",
       "4  you sir are my hero any chance you remember wh...   \n",
       "5  congratulations from me as well use the tools ...   \n",
       "6       cocksucker before you piss around on my work   \n",
       "7  your vandalism to the matt shirvington article...   \n",
       "8  sorry if the word nonsense was offensive to yo...   \n",
       "9  alignment on this subject and which are contra...   \n",
       "\n",
       "                                      tokenized_text  \n",
       "0  [explanation, why, the, edits, made, under, my...  \n",
       "1  [d, aww, he, matches, this, background, colour...  \n",
       "2  [hey, man, i, m, really, not, trying, to, edit...  \n",
       "3  [more, i, can, t, make, any, real, suggestions...  \n",
       "4  [you, sir, are, my, hero, any, chance, you, re...  \n",
       "5  [congratulations, from, me, as, well, use, the...  \n",
       "6  [cocksucker, before, you, piss, around, on, my...  \n",
       "7  [your, vandalism, to, the, matt, shirvington, ...  \n",
       "8  [sorry, if, the, word, nonsense, was, offensiv...  \n",
       "9  [alignment, on, this, subject, and, which, are...  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data2['tokenized_text'] = tokenized_text\n",
    "data2.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Лемматизация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напишем функцию, которая бы производила лемматизацию текста с помощьюю nltk WorldNetLemmatizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemm_text = \" \".join([lemmatizer.lemmatize(w) for w in text])\n",
    "    return lemm_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 23.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "lemm_text = tokenized_text.apply(lemmatize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         explanation why the edits made under my userna...\n",
       "1         d aww he match this background colour i m seem...\n",
       "2         hey man i m really not trying to edit war it s...\n",
       "3         more i can t make any real suggestion on impro...\n",
       "4         you sir are my hero any chance you remember wh...\n",
       "                                ...                        \n",
       "159566    and for the second time of asking when your vi...\n",
       "159567    you should be ashamed of yourself that is a ho...\n",
       "159568    spitzer umm there no actual article for prosti...\n",
       "159569    and it look like it wa actually you who put on...\n",
       "159570    and i really don t think you understand i came...\n",
       "Name: text, Length: 159571, dtype: object"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemm_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обратим внимание на то, что несмотря на произведенную лемматизацию многие слова не были приведены в начальную форму. Причина этого в том, что не проставлены Part-of-Speech теги, из-за этого лемматайзер плохо разбирает части речи и, соответственно, не знает, какая у слова начальная форма. Исправим это."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Пользователь\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1.22 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "def lemmatize_POS(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemm_POS_text = \" \".join([lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in text])\n",
    "    return lemm_POS_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 47min 8s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "lemm_text2 = tokenized_text.apply(lemmatize_POS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Невероятно долгая лемматизация. Посмотрим, насколько лучше стал лемматизирован текст."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2['lemm_text'] = lemm_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2['lemm_text_POS'] = lemm_text2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original</th>\n",
       "      <th>lemm_text</th>\n",
       "      <th>lemm_text_POS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>However, the Moonlite edit noted by golden dap...</td>\n",
       "      <td>however the moonlite edit noted by golden daph...</td>\n",
       "      <td>however the moonlite edit note by golden daph ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>Check the following websites:\\n\\nhttp://www.ir...</td>\n",
       "      <td>check the following website http www iranchamb...</td>\n",
       "      <td>check the follow website http www iranchamber ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>i can't believe no one has already put up this...</td>\n",
       "      <td>i can t believe no one ha already put up this ...</td>\n",
       "      <td>i can t believe no one have already put up thi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>\"\\n\\nWell, after I asked you to provide the di...</td>\n",
       "      <td>well after i asked you to provide the diffs wi...</td>\n",
       "      <td>well after i ask you to provide the diffs with...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>What page shoudld there be for important chara...</td>\n",
       "      <td>what page shoudld there be for important chara...</td>\n",
       "      <td>what page shoudld there be for important chara...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>A pair of jew-hating weiner nazi schmucks.</td>\n",
       "      <td>a pair of jew hating weiner nazi schmuck</td>\n",
       "      <td>a pair of jew hat weiner nazi schmuck</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>I tend to think that when the list is longer t...</td>\n",
       "      <td>i tend to think that when the list is longer t...</td>\n",
       "      <td>i tend to think that when the list be longer t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>\"\\n\\n What's up with this? \\n\"\"If you are a re...</td>\n",
       "      <td>what s up with this if you are a religiously o...</td>\n",
       "      <td>what s up with this if you be a religiously or...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>I'm not vandalizing  \\n\\nI'm just having fun m...</td>\n",
       "      <td>i m not vandalizing i m just having fun man yo...</td>\n",
       "      <td>i m not vandalize i m just have fun man you ha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>Welcome to Wikipedia ! [bla] Discover Ekopedia...</td>\n",
       "      <td>welcome to wikipedia bla discover ekopedia the...</td>\n",
       "      <td>welcome to wikipedia bla discover ekopedia the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>Including some appropriate mention of the Solo...</td>\n",
       "      <td>including some appropriate mention of the solo...</td>\n",
       "      <td>include some appropriate mention of the solomo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>\"\\n\\nComment. I could not verify the claim.  (...</td>\n",
       "      <td>comment i could not verify the claim talk</td>\n",
       "      <td>comment i could not verify the claim talk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>\"\\n Czech Republic is in Central Europe. The s...</td>\n",
       "      <td>czech republic is in central europe the state ...</td>\n",
       "      <td>czech republic be in central europe the state ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>Thanks, Josette. I enjoyed meeting you, too. I...</td>\n",
       "      <td>thanks josette i enjoyed meeting you too i wa ...</td>\n",
       "      <td>thanks josette i enjoy meeting you too i be sh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>Paleontologists agree that organic remains mus...</td>\n",
       "      <td>paleontologist agree that organic remains must...</td>\n",
       "      <td>paleontologist agree that organic remains must...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>Also I think Vegetable Basket needs it's own W...</td>\n",
       "      <td>also i think vegetable basket need it s own wi...</td>\n",
       "      <td>also i think vegetable basket need it s own wi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>Bigfoot Reference \\n\\nThe magazine is better k...</td>\n",
       "      <td>bigfoot reference the magazine is better known...</td>\n",
       "      <td>bigfoot reference the magazine be well know a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>Also see this if you cant trust Murkoth Ramunn...</td>\n",
       "      <td>also see this if you cant trust murkoth ramunn...</td>\n",
       "      <td>also see this if you cant trust murkoth ramunn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>\"\\n\\n Chart performance of \"\"Single Ladies (Pu...</td>\n",
       "      <td>chart performance of single lady put a ring on...</td>\n",
       "      <td>chart performance of single lady put a ring on...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>\"\\n\\nhahahaha.... good one ......\\nI have remo...</td>\n",
       "      <td>hahahaha good one i have removed it</td>\n",
       "      <td>hahahaha good one i have remove it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>\"\\n\\nHaving said that, I've temporarily remove...</td>\n",
       "      <td>having said that i ve temporarily removed my r...</td>\n",
       "      <td>have say that i ve temporarily remove my reque...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              original  \\\n",
       "100  However, the Moonlite edit noted by golden dap...   \n",
       "101  Check the following websites:\\n\\nhttp://www.ir...   \n",
       "102  i can't believe no one has already put up this...   \n",
       "103  \"\\n\\nWell, after I asked you to provide the di...   \n",
       "104  What page shoudld there be for important chara...   \n",
       "105         A pair of jew-hating weiner nazi schmucks.   \n",
       "106  I tend to think that when the list is longer t...   \n",
       "107  \"\\n\\n What's up with this? \\n\"\"If you are a re...   \n",
       "108  I'm not vandalizing  \\n\\nI'm just having fun m...   \n",
       "109  Welcome to Wikipedia ! [bla] Discover Ekopedia...   \n",
       "110  Including some appropriate mention of the Solo...   \n",
       "111  \"\\n\\nComment. I could not verify the claim.  (...   \n",
       "112  \"\\n Czech Republic is in Central Europe. The s...   \n",
       "113  Thanks, Josette. I enjoyed meeting you, too. I...   \n",
       "114  Paleontologists agree that organic remains mus...   \n",
       "115  Also I think Vegetable Basket needs it's own W...   \n",
       "116  Bigfoot Reference \\n\\nThe magazine is better k...   \n",
       "117  Also see this if you cant trust Murkoth Ramunn...   \n",
       "118  \"\\n\\n Chart performance of \"\"Single Ladies (Pu...   \n",
       "119  \"\\n\\nhahahaha.... good one ......\\nI have remo...   \n",
       "120  \"\\n\\nHaving said that, I've temporarily remove...   \n",
       "\n",
       "                                             lemm_text  \\\n",
       "100  however the moonlite edit noted by golden daph...   \n",
       "101  check the following website http www iranchamb...   \n",
       "102  i can t believe no one ha already put up this ...   \n",
       "103  well after i asked you to provide the diffs wi...   \n",
       "104  what page shoudld there be for important chara...   \n",
       "105           a pair of jew hating weiner nazi schmuck   \n",
       "106  i tend to think that when the list is longer t...   \n",
       "107  what s up with this if you are a religiously o...   \n",
       "108  i m not vandalizing i m just having fun man yo...   \n",
       "109  welcome to wikipedia bla discover ekopedia the...   \n",
       "110  including some appropriate mention of the solo...   \n",
       "111          comment i could not verify the claim talk   \n",
       "112  czech republic is in central europe the state ...   \n",
       "113  thanks josette i enjoyed meeting you too i wa ...   \n",
       "114  paleontologist agree that organic remains must...   \n",
       "115  also i think vegetable basket need it s own wi...   \n",
       "116  bigfoot reference the magazine is better known...   \n",
       "117  also see this if you cant trust murkoth ramunn...   \n",
       "118  chart performance of single lady put a ring on...   \n",
       "119                hahahaha good one i have removed it   \n",
       "120  having said that i ve temporarily removed my r...   \n",
       "\n",
       "                                         lemm_text_POS  \n",
       "100  however the moonlite edit note by golden daph ...  \n",
       "101  check the follow website http www iranchamber ...  \n",
       "102  i can t believe no one have already put up thi...  \n",
       "103  well after i ask you to provide the diffs with...  \n",
       "104  what page shoudld there be for important chara...  \n",
       "105              a pair of jew hat weiner nazi schmuck  \n",
       "106  i tend to think that when the list be longer t...  \n",
       "107  what s up with this if you be a religiously or...  \n",
       "108  i m not vandalize i m just have fun man you ha...  \n",
       "109  welcome to wikipedia bla discover ekopedia the...  \n",
       "110  include some appropriate mention of the solomo...  \n",
       "111          comment i could not verify the claim talk  \n",
       "112  czech republic be in central europe the state ...  \n",
       "113  thanks josette i enjoy meeting you too i be sh...  \n",
       "114  paleontologist agree that organic remains must...  \n",
       "115  also i think vegetable basket need it s own wi...  \n",
       "116  bigfoot reference the magazine be well know a ...  \n",
       "117  also see this if you cant trust murkoth ramunn...  \n",
       "118  chart performance of single lady put a ring on...  \n",
       "119                 hahahaha good one i have remove it  \n",
       "120  have say that i ve temporarily remove my reque...  "
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data2.loc[100:120,['original','lemm_text','lemm_text_POS']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Положа руку на сердце, заметим, что набор лемматизированных текстов стал несильно лучше. Но, как минимум, почти все глаголы встали в инфинитив. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Векторизация текстов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Перед тем, как приступить к обучению моделей и выбору лучшей, разобьем массив текстов на обучающий корпус, валидационную и тестовую выборки и векторизируем их. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Пользователь\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = set(nltk_stopwords.words('english'))\n",
    "count_tf_idf = TfidfVectorizer(max_features=5000, min_df=5, max_df=0.7, stop_words=stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemm_text_train_valid, lemm_text_test, target_train_valid, target_test = train_test_split(lemm_text2, data.toxic, \n",
    "                                                                                          random_state = state, \n",
    "                                                                                          test_size = 0.25)\n",
    "lemm_text_train, lemm_text_valid, target_train, target_valid = train_test_split(lemm_text_train_valid,\n",
    "                                                                                          target_train_valid, \n",
    "                                                                                          random_state = state, \n",
    "                                                                                          test_size = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemm_text_train_valid_noPOS, lemm_text_test_noPOS, target_train_valid_noPOS, target_test_noPOS = train_test_split(lemm_text, data.toxic, \n",
    "                                                                                          random_state = state, \n",
    "                                                                                          test_size = 0.25)\n",
    "lemm_text_train_noPOS, lemm_text_valid_noPOS, target_train_noPOS, target_valid_noPOS = train_test_split(lemm_text_train_valid_noPOS,\n",
    "                                                                                          target_train_valid_noPOS, \n",
    "                                                                                          random_state = state, \n",
    "                                                                                          test_size = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1.19 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "corpus_train = lemm_text_train.values.astype('U')\n",
    "corpus_valid = lemm_text_valid.values.astype('U')\n",
    "corpus_test = lemm_text_test.values.astype('U')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3.43 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "features_train = count_tf_idf.fit_transform(corpus_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1.39 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "features_valid = count_tf_idf.transform(corpus_valid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1.53 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "features_test = count_tf_idf.transform(corpus_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_train = features_train.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_valid = features_valid.toarray()\n",
    "features_test = features_test.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Основные корпусы текстов готовы. Однако мы проделаем то же самое и с текстом, лемматизированным без POS-меток, чтобы в дальнейшем сравнить качество не только алгоритмов, но и способов лемматизации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 11.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "corpus_train_noPOS = lemm_text_train_noPOS.values.astype('U')\n",
    "corpus_valid_noPOS = lemm_text_valid_noPOS.values.astype('U')\n",
    "corpus_test_noPOS = lemm_text_test_noPOS.values.astype('U')\n",
    "\n",
    "count_tf_idf_noPOS = TfidfVectorizer(max_features=5000, min_df=5, max_df=0.7, stop_words=stopwords)\n",
    "\n",
    "features_train_noPOS = count_tf_idf_noPOS.fit_transform(corpus_train).toarray()\n",
    "features_valid_noPOS = count_tf_idf_noPOS.transform(corpus_valid).toarray()\n",
    "features_test_noPOS = count_tf_idf_noPOS.transform(corpus_test).toarray()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Обучение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Логистическая регрессия"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 24.6 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Пользователь\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "log_reg.fit(features_train, target_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 251 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "log_reg_predicted_valid = log_reg.predict(features_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7325484994196652"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_reg_f1 = f1_score(target_valid, log_reg_predicted_valid)\n",
    "log_reg_f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Логистическая регрессия продемонстировала неплохой результат на лемматизированных с POS-метками текстах, однако он все-таки неудовлетворительный. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Пользователь\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7317397078353254"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_reg_noPOS = LogisticRegression()\n",
    "log_reg_noPOS.fit(features_train_noPOS, target_train_noPOS)\n",
    "log_reg_predicted_valid_noPOS = log_reg_noPOS.predict(features_valid_noPOS)\n",
    "log_reg_f1_noPOS = f1_score(target_valid_noPOS, log_reg_predicted_valid_noPOS)\n",
    "log_reg_f1_noPOS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Логистическая регрессия на данных без POS-меток продемонстрировала близкий, хоть и чуть худший результат."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Случайный лес"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def forest_search(features_train, target_train, features_valid, target_valid):\n",
    "    forest_depth_col = []\n",
    "    forest_estim_col = []\n",
    "    forest_score_col = []\n",
    "    for depth in [12, 30]:\n",
    "        for n_estim in [120, 200]:\n",
    "            forest_model = RandomForestClassifier(n_estimators = n_estim, \n",
    "                                                   max_depth = depth, \n",
    "                                                   random_state = state)\n",
    "            forest_model.fit(features_train, target_train)\n",
    "            forest_predicted_valid = forest_model.predict(features_valid)\n",
    "            forest_score = f1_score(target_valid, forest_predicted_valid)\n",
    "            forest_depth_col.append(depth)\n",
    "            forest_estim_col.append(n_estim)\n",
    "            forest_score_col.append(forest_score)\n",
    "    forest_hyperparameters_dict = {'max_depth': forest_depth_col, 'n_estimators': forest_estim_col, 'f1_score': forest_score_col}\n",
    "    forest_hyperparameters = pd.DataFrame(data = forest_hyperparameters_dict)\n",
    "    return forest_hyperparameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 4min 17s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>max_depth</th>\n",
       "      <th>n_estimators</th>\n",
       "      <th>f1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>30</td>\n",
       "      <td>200</td>\n",
       "      <td>0.364735</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   max_depth  n_estimators  f1_score\n",
       "0         30           200  0.364735"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "forest_results = forest_search(features_train, target_train, features_valid, target_valid)\n",
    "forest_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Случайный лес продемонстрировал очень низкий показатель, значительно ниже, чем логистическая регрессия."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CatBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем применить алгоритм градиентного бустинга. Для начала - пробный вариант."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate set to 0.476972\n",
      "0:\tlearn: 0.3443652\ttotal: 660ms\tremaining: 1m 18s\n",
      "10:\tlearn: 0.1846563\ttotal: 2.48s\tremaining: 24.6s\n",
      "20:\tlearn: 0.1646103\ttotal: 4.05s\tremaining: 19.1s\n",
      "30:\tlearn: 0.1499102\ttotal: 5.61s\tremaining: 16.1s\n",
      "40:\tlearn: 0.1411258\ttotal: 7.16s\tremaining: 13.8s\n",
      "50:\tlearn: 0.1345853\ttotal: 8.74s\tremaining: 11.8s\n",
      "60:\tlearn: 0.1287429\ttotal: 10.3s\tremaining: 9.99s\n",
      "70:\tlearn: 0.1249347\ttotal: 11.9s\tremaining: 8.19s\n",
      "80:\tlearn: 0.1217940\ttotal: 13.5s\tremaining: 6.49s\n",
      "90:\tlearn: 0.1183494\ttotal: 15s\tremaining: 4.79s\n",
      "100:\tlearn: 0.1148458\ttotal: 16.6s\tremaining: 3.13s\n",
      "110:\tlearn: 0.1121057\ttotal: 18.2s\tremaining: 1.48s\n",
      "119:\tlearn: 0.1102410\ttotal: 19.6s\tremaining: 0us\n",
      "Wall time: 58 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<catboost.core.CatBoostClassifier at 0x234a89c1988>"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "catbo = CatBoostClassifier(loss_function=\"Logloss\", iterations=120)\n",
    "catbo.fit(features_train, target_train, verbose=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 16.7 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7394796016704143"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "catbo_predicted_valid = catbo.predict(features_valid)\n",
    "catbo_f1 = f1_score(target_valid, catbo_predicted_valid)\n",
    "catbo_f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Результат довольно близок к требуемому. Благо, мы можем попробовать другие гиперпараметры, чтобы преодолеть требуемый порог качества."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def CatBoost_search(features_train, target_train, features_valid, target_valid):\n",
    "    learning_rate_col = []\n",
    "    iterations_col = []\n",
    "    f1_score_col = []\n",
    "    l2_reg_col = []\n",
    "    for learn_rate in [0.6,0.7]:\n",
    "        for l2_leaf_reg in [8,12]:\n",
    "            for iterations in [190, 200]:\n",
    "                catboost_model = CatBoostClassifier(loss_function = 'Logloss',\n",
    "                                                       iterations = iterations,\n",
    "                                                       learning_rate = learn_rate, \n",
    "                                                    l2_leaf_reg = l2_leaf_reg,\n",
    "                                                       random_state = state, \n",
    "                                                       verbose = False)\n",
    "                catboost_model.fit(features_train, \n",
    "                                       target_train, \n",
    "                                       verbose=False)\n",
    "                catboost_predicted_valid = catboost_model.predict(features_valid)\n",
    "                catbo_score = f1_score(target_valid, catboost_predicted_valid)\n",
    "                l2_reg_col.append(l2_leaf_reg)\n",
    "                learning_rate_col.append(learn_rate)\n",
    "                iterations_col.append(iterations)\n",
    "                f1_score_col.append(catbo_score)\n",
    "    catboost_hyperparameters_dict = {'learning_rate': learning_rate_col, \n",
    "                                     'iterations': iterations_col, \n",
    "                                     'l2_leaf_reg': l2_reg_col,\n",
    "                                     'f1_score': f1_score_col}\n",
    "    catboost_hyperparameters = pd.DataFrame(data = catboost_hyperparameters_dict)\n",
    "    return catboost_hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 12min 59s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>iterations</th>\n",
       "      <th>l2_leaf_reg</th>\n",
       "      <th>f1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.7</td>\n",
       "      <td>200</td>\n",
       "      <td>12</td>\n",
       "      <td>0.755424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.6</td>\n",
       "      <td>190</td>\n",
       "      <td>8</td>\n",
       "      <td>0.755381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.7</td>\n",
       "      <td>190</td>\n",
       "      <td>12</td>\n",
       "      <td>0.754528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.6</td>\n",
       "      <td>200</td>\n",
       "      <td>8</td>\n",
       "      <td>0.753610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.6</td>\n",
       "      <td>200</td>\n",
       "      <td>12</td>\n",
       "      <td>0.749058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.7</td>\n",
       "      <td>200</td>\n",
       "      <td>8</td>\n",
       "      <td>0.748985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.6</td>\n",
       "      <td>190</td>\n",
       "      <td>12</td>\n",
       "      <td>0.748901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.7</td>\n",
       "      <td>190</td>\n",
       "      <td>8</td>\n",
       "      <td>0.748280</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   learning_rate  iterations  l2_leaf_reg  f1_score\n",
       "7            0.7         200           12  0.755424\n",
       "0            0.6         190            8  0.755381\n",
       "6            0.7         190           12  0.754528\n",
       "1            0.6         200            8  0.753610\n",
       "3            0.6         200           12  0.749058\n",
       "5            0.7         200            8  0.748985\n",
       "2            0.6         190           12  0.748901\n",
       "4            0.7         190            8  0.748280"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "catbo_results = CatBoost_search(features_train, target_train, features_valid, target_valid)\n",
    "catbo_results.sort_values(by = 'f1_score', ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>iterations</th>\n",
       "      <th>l2_leaf_reg</th>\n",
       "      <th>f1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.7</td>\n",
       "      <td>200</td>\n",
       "      <td>12</td>\n",
       "      <td>0.755424</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   learning_rate  iterations  l2_leaf_reg  f1_score\n",
       "7            0.7         200           12  0.755424"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "catbo_best = catbo_results.sort_values(by = 'f1_score', ascending = False).head(1)\n",
    "catbo_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "catbo_best_rate = float(catbo_best.learning_rate)\n",
    "catbo_best_iterations = int(catbo_best.iterations)\n",
    "catbo_best_reg = int(catbo_best.l2_leaf_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 14min 38s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>iterations</th>\n",
       "      <th>l2_leaf_reg</th>\n",
       "      <th>f1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.6</td>\n",
       "      <td>190</td>\n",
       "      <td>8</td>\n",
       "      <td>0.745500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.6</td>\n",
       "      <td>200</td>\n",
       "      <td>8</td>\n",
       "      <td>0.746250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.6</td>\n",
       "      <td>190</td>\n",
       "      <td>12</td>\n",
       "      <td>0.750988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.6</td>\n",
       "      <td>200</td>\n",
       "      <td>12</td>\n",
       "      <td>0.750672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.7</td>\n",
       "      <td>190</td>\n",
       "      <td>8</td>\n",
       "      <td>0.750705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.7</td>\n",
       "      <td>200</td>\n",
       "      <td>8</td>\n",
       "      <td>0.750352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.7</td>\n",
       "      <td>190</td>\n",
       "      <td>12</td>\n",
       "      <td>0.752225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.7</td>\n",
       "      <td>200</td>\n",
       "      <td>12</td>\n",
       "      <td>0.752225</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   learning_rate  iterations  l2_leaf_reg  f1_score\n",
       "0            0.6         190            8  0.745500\n",
       "1            0.6         200            8  0.746250\n",
       "2            0.6         190           12  0.750988\n",
       "3            0.6         200           12  0.750672\n",
       "4            0.7         190            8  0.750705\n",
       "5            0.7         200            8  0.750352\n",
       "6            0.7         190           12  0.752225\n",
       "7            0.7         200           12  0.752225"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "catbo_results_noPOS = CatBoost_search(features_train_noPOS, target_train_noPOS, features_valid_noPOS, target_valid_noPOS)\n",
    "catbo_results_noPOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>iterations</th>\n",
       "      <th>l2_leaf_reg</th>\n",
       "      <th>f1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.7</td>\n",
       "      <td>190</td>\n",
       "      <td>12</td>\n",
       "      <td>0.752225</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   learning_rate  iterations  l2_leaf_reg  f1_score\n",
       "6            0.7         190           12  0.752225"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "catbo_results_noPOS.sort_values(by = 'f1_score', ascending = False).head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Модели на лемматизированных без меток данных показали удовлетворительное качество на валидационной выборке, уступая моделям на данных с POS-тегами самую малость. Впрочем, именно этой небольшой разницы может не хватить для удовлетворительного результата на тестовой выборке (на ней качество обычно ниже по сравнению с валидационной), поэтому тестировать будем модель CatBoost на данных с POS-тегами."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Тестирование"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "У нас есть модель, которая показала удовлетворительное качество на валидационном массиве. Протестируем ее."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.2813201\ttotal: 231ms\tremaining: 46s\n",
      "10:\tlearn: 0.1729919\ttotal: 2.31s\tremaining: 39.7s\n",
      "20:\tlearn: 0.1526421\ttotal: 4.38s\tremaining: 37.3s\n",
      "30:\tlearn: 0.1400870\ttotal: 6.43s\tremaining: 35.1s\n",
      "40:\tlearn: 0.1325819\ttotal: 8.46s\tremaining: 32.8s\n",
      "50:\tlearn: 0.1275616\ttotal: 10.5s\tremaining: 30.6s\n",
      "60:\tlearn: 0.1242400\ttotal: 12.5s\tremaining: 28.5s\n",
      "70:\tlearn: 0.1222648\ttotal: 14.5s\tremaining: 26.4s\n",
      "80:\tlearn: 0.1200765\ttotal: 16.6s\tremaining: 24.3s\n",
      "90:\tlearn: 0.1157384\ttotal: 18.7s\tremaining: 22.4s\n",
      "100:\tlearn: 0.1123619\ttotal: 20.8s\tremaining: 20.3s\n",
      "110:\tlearn: 0.1109482\ttotal: 22.8s\tremaining: 18.3s\n",
      "120:\tlearn: 0.1099555\ttotal: 24.9s\tremaining: 16.2s\n",
      "130:\tlearn: 0.1085034\ttotal: 26.9s\tremaining: 14.2s\n",
      "140:\tlearn: 0.1070719\ttotal: 29s\tremaining: 12.1s\n",
      "150:\tlearn: 0.1065479\ttotal: 31s\tremaining: 10.1s\n",
      "160:\tlearn: 0.1058025\ttotal: 33s\tremaining: 8s\n",
      "170:\tlearn: 0.1050202\ttotal: 35s\tremaining: 5.94s\n",
      "180:\tlearn: 0.1043385\ttotal: 37s\tremaining: 3.89s\n",
      "190:\tlearn: 0.1032147\ttotal: 39.1s\tremaining: 1.84s\n",
      "199:\tlearn: 0.1024255\ttotal: 40.9s\tremaining: 0us\n",
      "Wall time: 1min 41s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7524177949709864"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "catbo_best = CatBoostClassifier(loss_function=\"Logloss\", l2_leaf_reg = catbo_best_reg, learning_rate = catbo_best_rate, iterations=catbo_best_iterations)\n",
    "catbo_best.fit(features_train, target_train, verbose=10)\n",
    "predicted_test = catbo_best.predict(features_test)\n",
    "f1_test = f1_score(target_test, predicted_test)\n",
    "f1_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "F1 на тестовой выборке больше 0.75, а значит, требования заказчика выполнены. Однако мы можем обучить модель на корпусе, слитом из обучающего и валидационного, чтобы посмотреть весь потенциал модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_train_valid = count_tf_idf.transform(lemm_text_train_valid.values.astype('U')).toarray()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.2886323\ttotal: 582ms\tremaining: 1m 55s\n",
      "10:\tlearn: 0.1719158\ttotal: 3.67s\tremaining: 1m 3s\n",
      "20:\tlearn: 0.1518512\ttotal: 6.46s\tremaining: 55.1s\n",
      "30:\tlearn: 0.1407321\ttotal: 9.25s\tremaining: 50.5s\n",
      "40:\tlearn: 0.1331755\ttotal: 12.1s\tremaining: 46.8s\n",
      "50:\tlearn: 0.1277977\ttotal: 14.9s\tremaining: 43.6s\n",
      "60:\tlearn: 0.1237537\ttotal: 17.8s\tremaining: 40.5s\n",
      "70:\tlearn: 0.1218356\ttotal: 20.6s\tremaining: 37.4s\n",
      "80:\tlearn: 0.1190993\ttotal: 23.4s\tremaining: 34.4s\n",
      "90:\tlearn: 0.1164301\ttotal: 26.2s\tremaining: 31.3s\n",
      "100:\tlearn: 0.1145110\ttotal: 28.9s\tremaining: 28.4s\n",
      "110:\tlearn: 0.1119770\ttotal: 31.7s\tremaining: 25.4s\n",
      "120:\tlearn: 0.1104388\ttotal: 34.6s\tremaining: 22.6s\n",
      "130:\tlearn: 0.1091784\ttotal: 37.4s\tremaining: 19.7s\n",
      "140:\tlearn: 0.1082627\ttotal: 40.1s\tremaining: 16.8s\n",
      "150:\tlearn: 0.1074990\ttotal: 42.9s\tremaining: 13.9s\n",
      "160:\tlearn: 0.1064795\ttotal: 45.7s\tremaining: 11.1s\n",
      "170:\tlearn: 0.1060333\ttotal: 48.5s\tremaining: 8.23s\n",
      "180:\tlearn: 0.1052480\ttotal: 51.3s\tremaining: 5.39s\n",
      "190:\tlearn: 0.1049272\ttotal: 54s\tremaining: 2.55s\n",
      "199:\tlearn: 0.1043903\ttotal: 56.5s\tremaining: 0us\n",
      "Wall time: 2min 14s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7564752638070439"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "catbo_best2 = CatBoostClassifier(loss_function=\"Logloss\", l2_leaf_reg = catbo_best_reg, learning_rate = catbo_best_rate, iterations=catbo_best_iterations)\n",
    "catbo_best2.fit(features_train_valid, target_train_valid, verbose=10)\n",
    "predicted_test2 = catbo_best2.predict(features_test)\n",
    "f1_test2 = f1_score(target_test, predicted_test2)\n",
    "f1_test2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Качество предсказания лучше, если обучить модель CatBoost на корпусе из обучающей и валидационной выборки, хоть и не очень сильно."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Выводы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы выполнили требование заказчика с помощью модели градиентного бустинга на алгоритме CatBoost, обученной на предобработанных - очищенных, токенизированных и лемматизированных с POS-тегами - данных.  \n",
    "Можно отметить, что качество моделей на данных с POS-тегами выше, чем у моделей на данных, лемматизированных без POS-тегов, но не сильно. Можно предположить, что лемматизация в данном случае - а мы работали с английским языком, в котором в тексте используется сравнительно меньше словоформ, чем в других языках, как минимум из-за отсутствия падежей - играла не столь значительную роль, за счет этого и разница между лучше и хуже лемматизированными текстами не давала различий в качестве.  \n",
    "Однако нельзя исключать, что лемматизация во втором случае, даже с POS-тегами прошла не лучшим образом, некоторые слова все равно не были возвращены в начальную форму. Возможно, на это оказала влияние очистка корпуса, которая могла помимо мешающих элементов исключить и важные, способствующие определению формы слова и части речи симоволы, например, апостроф.  \n",
    "\n",
    "Тем не менее, требование заказчика выполнено. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
